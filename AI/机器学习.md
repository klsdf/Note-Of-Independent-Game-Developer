在机器学习领域，根据预测的目标变量的类型，主要可以将问题分为两大类：回归问题（Regression）和分类问题（Classification）。

1. **回归问题（Regression）**：预测股票
    
    - **目标**：预测一个连续的数值。例如，预测房价、股票价格、温度等。
    - **方法**：使用的算法可能包括线性回归、多项式回归、决策树回归、随机森林回归、支持向量回归（SVR）等。
    - **评估指标**：常用的评估指标包括均方误差（MSE）、均方根误差（RMSE）、绝对误差（MAE）等。
2. **分类问题（Classification）**：alphago下围棋
    - **目标**：预测一个离散的类别标签。例如，判断邮件是否为垃圾邮件、识别图片中的对象类别（如猫、狗等）、病人诊断结果（病或者健康）等。
    - **方法**：使用的算法可能包括逻辑回归、决策树、随机森林、支持向量机（SVM）、神经网络等。
    - **评估指标**：常用的评估指标包括准确率（Accuracy）、精确率（Precision）、召回率（Recall）、F1分数、混淆矩阵等。

这两类问题是机器学习中最基本的任务类型，选择哪种类型主要取决于数据特性和预测目标。通过对数据进行训练，机器学习模型能够学习数据模式，进而对新的未见过的数据进行准确的预测或分类。

结构学习（Structure Learning）也是机器学习中的一个重要问题，尤其在概率图模型（Probabilistic Graphical Models）和一些深度学习模型中尤为重要。结构学习的目标是从数据中学习出模型的结构，这种结构通常表示变量之间的依赖关系或者是数据内在的组织方式。
让ai写文章作曲等。




机器学习本质是去拟合一个函数。
假如用$y= b+ wx_1$去拟合的话，b和w就是未知的参数。w被叫做**weight**，b叫做**bias**。
这个函数就叫**model**，而x和y的数据是有数据集的，这些已知的参数叫做**feature**。

Loss(b,w)是损失函数，当b和w取得某一个值的时候，计算出来的y和数据集相差的数值。
$L=\frac{1}{N}\sum_{i=1}^N|y_N^`-y_N|$
当L是用绝对值算出来的，就叫mean absolute error（MAE）


$L=\frac{1}{N}\sum_{i=1}^N(y_N^`-y_N)^2$

当L是用平方算出来的，就叫mean square error (MSE)


理论上是有一组值$(b^*,w^*)$使得L最小，为了寻找这组值，可以使用梯度下降（Gradient Descent）方法。固定其他参数，当某一个参数变化时计算其Loss函数。当某点的斜率是正数时，说明最低点在左边。斜率为负，说明最低点在右边。然后移动参数的坐标，就可以不断逼近最低点。
 $$\left. \frac{{\rm d}L}{{\rm d}w} \right|_{w=w_0} $$
假如w=-2，此时发现微分的结果是−13，也就是负数，此时x减去这个微分的结果，-2-(-13)=11，就可以向右前进了。


```functionplot
---
title: 不同w的loss函数
xLabel: w
yLabel: Loss捏
bounds: [-10, 10, -20, 30]
disbaleZoom: 1
grid: true
---
g(x) =  -13x-9
f(x) = 2x^2-5x-1
```


但是这样前进的步长过大，因此还需要一个参数来限制。因此最终步长为。

 $$ \eta \left. \frac{{\rm d}L}{{\rm d}w} \right|_{w=w_0} $$

其中，eta 代表学习率（learning rate）


不过这种方法会有**local minima**的问题，也就是会走到一个局部最小值，而没有到达全局最小值（**global minima**）





实际上的函数也许非常复杂，如果使用$y= b+ wx_1$，这样的线性方程去拟合，可能会导致较大误差
![[Pasted image 20240418114841.png]]
这种来自于model的限制就叫**model bias**


对于这种复杂的函数，可以使用一些简单的函数去逼近。
![[Pasted image 20240418115302.png]]
像这样的蓝色函数就叫做sigmoid，方程式写法如下
$$

 \sigma(x)=\frac{1}{1+e^{-(b+wx)}}

$$
拟合后的红色线就可以表述为：
$$

 y = b +\sum_{i=1}^N c_i sigmoid(b_i+\sum_{j=1}^Nw_{ij}x_j)

$$
这样，上一层的$y= b+ wx_1$就变成了新的自变量。之前单一的直线就变成了这样的曲曲线。

![[Pasted image 20240418143039.png]]


网络上一层的值进行sigmoid
![[Pasted image 20240418143340.png]]


![[Pasted image 20240418143444.png]]

把所有的未知参数都作为输入，就可以得到一个θ向量，
![[Pasted image 20240418144424.png]]


θ向量在每一层网络的导数就是梯度，用下面的符号表示。
![[Pasted image 20240418144026.png]]
这样就可以得到各个参数在某一个值时的梯度。




如果训练集数量过大，算loss可能会很消耗时间，所以可以把数据集分成多个batch。在每一个batch中选一个数据来计算loss。
先用第一个batch中的一个数据算梯度，然后更新了之后继续下一个batch。
![[Pasted image 20240418144926.png]]
把数据全部扫一遍之后就叫过了一遍epoch





对于上面的数据实际上也没有必要一定使用sigmoid，也可以使用relu函数
![[Pasted image 20240418145311.png]]
两个RelU叠加之后也可以得到sigmoid

![[Pasted image 20240418145658.png]]


这些使得之前的线性方程变得歪七扭八，弯弯曲曲的函数就叫激活函数（Activation Function）



将这些计算过程起名叫做神经网络。
下图就是一个全连接的神经网络。每一个神经元都要观看完整的数据（x1，x2，，xn）
![[Pasted image 20240418150117.png]]





## overfitting

训练好的模型，如果在测试数据上准确率很高，而预测数据上准确率很低。很有可能就是过拟合（overfitting）了。
![[Pasted image 20240418150715.png]]
过拟合的模型在测试数据上都很准，但是其他时候，函数大概率就放飞自我了。会变成奇形怪状。这是因为模型的弹性过大。有可能是层数太深导致的。

## CNN

CNN可以用来做图像分类。

一个图像有rgb三通道，假如图片有512像素。如果把这些参数都作为输入的话，输入的参数就有$512*512*3$个参数，如果再做全连接网络的话，大概率是跑不动的。

可以利用图像的特性来简化网络。人类在辨识一个物体的时候，只需要根据一些特征就能分辨出来。比如傲娇角色必定是金发青梅竹马。

所以这样一来，一个neuron就不需要看完整的图片了。每个neuron只需要关心自己的感受野（receptive field）。例如蓝色的神经元只关心左上角的区域。不同神经元的感受野可以重合。

![[Pasted image 20240418152810.png]]


感受野的大小叫做kernel size，上图大小就是3\*3

一般不同神经元的范围是有重合的，两个神经元直接的间距就是步长（stride）

![[Pasted image 20240418153217.png]]
当感受野的范围超出了数据范围的时候，可以补充数据。比如全部补0，补1等，有很多方式。