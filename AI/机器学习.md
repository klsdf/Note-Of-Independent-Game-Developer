在机器学习领域，根据预测的目标变量的类型，主要可以将问题分为两大类：回归问题（Regression）和分类问题（Classification）。

1. **回归问题（Regression）**：预测股票
    
    - **目标**：预测一个连续的数值。例如，预测房价、股票价格、温度等。
    - **方法**：使用的算法可能包括线性回归、多项式回归、决策树回归、随机森林回归、支持向量回归（SVR）等。
    - **评估指标**：常用的评估指标包括均方误差（MSE）、均方根误差（RMSE）、绝对误差（MAE）等。
2. **分类问题（Classification）**：alphago下围棋
    - **目标**：预测一个离散的类别标签。例如，判断邮件是否为垃圾邮件、识别图片中的对象类别（如猫、狗等）、病人诊断结果（病或者健康）等。
    - **方法**：使用的算法可能包括逻辑回归、决策树、随机森林、支持向量机（SVM）、神经网络等。
    - **评估指标**：常用的评估指标包括准确率（Accuracy）、精确率（Precision）、召回率（Recall）、F1分数、混淆矩阵等。

这两类问题是机器学习中最基本的任务类型，选择哪种类型主要取决于数据特性和预测目标。通过对数据进行训练，机器学习模型能够学习数据模式，进而对新的未见过的数据进行准确的预测或分类。

结构学习（Structure Learning）也是机器学习中的一个重要问题，尤其在概率图模型（Probabilistic Graphical Models）和一些深度学习模型中尤为重要。结构学习的目标是从数据中学习出模型的结构，这种结构通常表示变量之间的依赖关系或者是数据内在的组织方式。
让ai写文章作曲等。






机器学习本质是去拟合一个函数。
假如用$y= b+ wx_1$去拟合的话，b和w就是未知的参数。w被叫做**weight**，b叫做**bias**。
这个函数就叫**model**，而x和y的数据是有数据集的，这些已知的参数叫做**feature**。

Loss(b,w)是损失函数，当b和w取得某一个值的时候，计算出来的y和数据集相差的数值。
$L=\frac{1}{N}\sum_{i=1}^N|y_N^`-y_N|$
当L是用绝对值算出来的，就叫mean absolute error（MAE）


$L=\frac{1}{N}\sum_{i=1}^N(y_N^`-y_N)^2$

当L是用平方算出来的，就叫mean square error (MSE)

![[Pasted image 20240419170113.png]]

固定其他参数时，只改变b，当刚好过线的时候，导数为0，最低点。


## 梯度下降算法

理论上是有一组值$(b^*,w^*)$使得L最小，为了寻找这组值，可以使用梯度下降（Gradient Descent）方法。固定其他参数，当某一个参数变化时计算其Loss函数。当某点的斜率是正数时，说明最低点在左边。斜率为负，说明最低点在右边。然后移动参数的坐标，就可以不断逼近最低点。
 $$\left. \frac{{\rm d}L}{{\rm d}w} \right|_{w=w_0} $$
假如w=-2，此时发现微分的结果是−13，也就是负数，此时x减去这个微分的结果，-2-(-13)=11，就可以向右前进了。


```functionplot
---
title: 不同w的loss函数
xLabel: w
yLabel: Loss捏
bounds: [-10, 10, -20, 30]
disbaleZoom: 1
grid: true
---
g(x) =  -13x-9
f(x) = 2x^2-5x-1
```


但是这样前进的步长过大，因此还需要一个参数来限制。因此最终步长为。

 $$ \eta \left. \frac{{\rm d}L}{{\rm d}w} \right|_{w=w_0} $$

其中，eta 代表学习率（learning rate）


不过这种方法会有**local minima**的问题，也就是会走到一个局部最小值，而没有到达全局最小值（**global minima**）





实际上的函数也许非常复杂，如果使用$y= b+ wx_1$，这样的线性方程去拟合，可能会导致较大误差
![[Pasted image 20240418114841.png]]
这种来自于model的限制就叫**model bias**


对于这种复杂的函数，可以使用一些简单的函数去逼近。
![[Pasted image 20240418115302.png]]
像这样的蓝色函数就叫做sigmoid，方程式写法如下
$$

 \sigma(x)=\frac{1}{1+e^{-(b+wx)}}

$$
拟合后的红色线就可以表述为：
$$

 y = b +\sum_{i=1}^N c_i sigmoid(b_i+\sum_{j=1}^Nw_{ij}x_j)

$$
这样，上一层的$y= b+ wx_1$就变成了新的自变量。之前单一的直线就变成了这样的曲曲线。

![[Pasted image 20240418143039.png]]


网络上一层的值进行sigmoid
![[Pasted image 20240418143340.png]]


![[Pasted image 20240418143444.png]]



为了方便起见，可以用矩阵表示这个运算。

![[Pasted image 20240423153415.png]]


【中国翻訳】莲华老公 16:22:32  
神经元越多，越能拟合复杂的函数  
  
【中国翻訳】莲华老公 16:22:41  
但是层数越多模型的弹性才能上去  
  
【中国翻訳】莲华老公 16:23:44  
激活函数本质上就是对一组线性变换做曲曲扭扭化  
  
【中国翻訳】莲华老公 16:24:10  
使得其线性方程映射成扭扭子方程


把所有的未知参数都作为输入，就可以得到一个θ向量，
![[Pasted image 20240418144424.png]]


θ向量在每一层网络的导数就是梯度，用下面的符号表示。
![[Pasted image 20240418144026.png]]
这样就可以得到各个参数在某一个值时的梯度。




如果训练集数量过大，算loss可能会很消耗时间，所以可以把数据集分成多个batch。在每一个batch中选一个数据来计算loss。
先用第一个batch中的一个数据算梯度，然后更新了之后继续下一个batch。
![[Pasted image 20240418144926.png]]
把数据全部扫一遍之后就叫过了一遍epoch





对于上面的数据实际上也没有必要一定使用sigmoid，也可以使用relu函数
![[Pasted image 20240418145311.png]]
两个RelU叠加之后也可以得到sigmoid

![[Pasted image 20240418145658.png]]


这些使得之前的线性方程变得歪七扭八，弯弯曲曲的函数就叫激活函数（Activation Function）

之所以每一层都需要多个激活函数，是因为一个激活函数是一个曲线，只有多个函数加起来，才能拟合真正的函数曲线。
![[Pasted image 20240422150740.png]]





将这些计算过程起名叫做神经网络。
下图就是一个全连接的神经网络。每一个神经元都要观看完整的数据（x1，x2，，xn）
![[Pasted image 20240418150117.png]]





## overfitting
训练好的模型，如果在测试数据上准确率很高，而预测数据上准确率很低。很有可能就是过拟合（overfitting）了。
![[Pasted image 20240418150715.png]]
过拟合的模型在测试数据上都很准，但是其他时候，函数大概率就放飞自我了。会变成奇形怪状。这是因为模型的弹性过大。有可能是层数太深导致的。



## 反向传播


```mermaid
flowchart LR 


x1-->|w1|hidden1x-->|sigmoid|hidden1y-->|w5|y
x2-->|w2|hidden1x
x1-->|w3|hidden2x-->|sigmoid|hidden2y-->|w6|y
x2-->|w4|hidden2x

```




$hidden1x= x1*w1+x2*w2$
$hidden1y= sigmoid(hidden1x)$
$y= w5*hidden1y +w6*hidden2y$
$L= 0.5* (y-trueY)^2$

因此计算w5的更新时，需要求出损失函数L对w5的偏导数
$$
\frac{ \mathrm{d}L}{\mathrm{d}w_5} = \frac{ \mathrm{d}L}{\mathrm{d}y}* \frac{ \mathrm{d}y}{\mathrm{d}w_5}  = (y-trueY)*hidden1y
$$

w5更新之后的值为
$$
w^+_5 =w_5-\eta\frac{ \mathrm{d}L}{\mathrm{d}w_5}   
$$


计算w1更新时，需要计算损失函数对w1的偏导数

$$
\begin{align}

\frac{ \mathrm{d}L}{\mathrm{d}w_1} = \frac{ \mathrm{d}L}{\mathrm{d}y}* \frac{ \mathrm{d}y}{\mathrm{d}hidden1y} * \frac{ \mathrm{d}hidden1y}{\mathrm{d}hidden1x} * \frac{ \mathrm{d}hidden1x}{\mathrm{d}w_1} \\ = (y-trueY)*w5*sigmoid(hidden1x)(1-sigmoid(hidden1x))*x_1

\end{align}
$$

## CNN

CNN可以用来做图像分类。

一个图像有rgb三通道，假如图片有512像素。如果把这些参数都作为输入的话，输入的参数就有$512*512*3$个参数，如果再做全连接网络的话，大概率是跑不动的。

可以利用图像的特性来简化网络。人类在辨识一个物体的时候，只需要根据一些特征就能分辨出来。比如傲娇角色必定是金发青梅竹马。

所以这样一来，一个neuron就不需要看完整的图片了。每个neuron只需要关心自己的感受野（receptive field）。例如蓝色的神经元只关心左上角的区域。不同神经元的感受野可以重合。

![[Pasted image 20240418152810.png]]


感受野的大小叫做kernel size，上图大小就是3\*3

一般不同神经元的范围是有重合的，两个神经元直接的间距就是步长（stride）

![[Pasted image 20240418153217.png]]
当感受野的范围超出了数据范围的时候，可以补充数据。比如全部补0，补1等，有很多方式。

不同的神经元会公用参数，这组参数就是w向量。来表示输入的权重。公用的参数也可能有很多组，这些公用的权重就叫filter。

filter实际上就是算子
![[Pasted image 20240418154218.png]]


不同的神经元在各自的感受野，用这个filter和感受野的矩阵做点积。就可以得到右边的矩阵。这个矩阵就能保留原来图像的特征。


![[Pasted image 20240418154331.png]]

比如如果出现了对角线的111，那么得到的结果就是3，只要有3，就代表会出现这个特征。
这个过程就是Convolution



pooling

数据集还是太大的时候，每一层的数据也会非常多。而把图像的像素间隔删除的时候，不影响特征表达。这样可以减少数据量。
因此在多层的网络上，可以每次选择一个值作为下一次的输入。![[Pasted image 20240418160635.png]]
![[Pasted image 20240418160416.png]]





![[Pasted image 20240418161246.png]]


注意，并不一定要polling。比如alpha go的围棋，如果去除一些行列的话，会导致棋局发生变化，并不能像图片一样，去除行列还是和之前一样。

cnn也无法处理旋转问题。一个狗转一圈就认不出来了了。





## self attention
当输入的数据不是一个向量，而是一组向量时。就需要新的网络模型了。
比如处理音频的时候，实际上就是处理了多个向量。将25ms的数据进行处理。






## 图像评估
![[Pasted image 20240430143551.png]]




![[Pasted image 20240430144520.png]]